{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment analysis of text data\n",
    "\n",
    "The focus for this lab is classification of natural language data, we'll be using the [Movie Review Polarity Dataset](http://www.cs.cornell.edu/people/pabo/movie-review-data/) that includes film reviews annotated with a label that classify them as positive or negative. The task is to build a classifier to predict new (unseen) reviews.\n",
    "\n",
    "The usual workflow for building and deploying a classifier is depicted in the image below\n",
    "\n",
    "![Text Classification workflow](https://developers.google.com/machine-learning/guides/text-classification/images/Workflow.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we prepare the computing environment by importing the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import warnings\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [NLTK](https://www.nltk.org/) installed package doesn't include the necessary data which should be installed as described in the [documentation](https://www.nltk.org/data.html). The full list of available corpora data is available on [NLTK website](https://www.nltk.org/nltk_data/).\n",
    "\n",
    "In our case we just need the stopwords, which can be downloaded as following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\black\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\black\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect and load data\n",
    "\n",
    "The dataset is in a zip archive `data/review_polarity.zip`, in the archive reviews are stored within the `txt_sentoken` directory as single files. Each file corresponding to a single review is stored in the `pos` or `neg` subdirectory according to its classification. The function below will load the dataset in a pandas dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zipfile import ZipFile\n",
    "import re\n",
    "\n",
    "def load_dataset_archive(ziparch, seed=None, encoding='utf-8'):\n",
    "    \"\"\"Load the Movie Review Polarity Dataset from the given zip archive.\n",
    "    For the description of the data see <http://www.cs.cornell.edu/people/pabo/movie-review-data/>\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    with ZipFile(ziparch, 'r') as myzip:\n",
    "        for fi in myzip.infolist():\n",
    "            if not fi.is_dir():\n",
    "                m = re.search('/(neg|pos)/(\\w+).txt$', fi.filename)\n",
    "                if m:\n",
    "                    row = {'id': m.group(2), 'Text': myzip.read(fi).decode(encoding), 'Label': 0 if m.group(1) == 'neg' else 1}\n",
    "                    data.append(row)\n",
    "\n",
    "    # shuffle data to avoid order biases\n",
    "    random.seed(seed)\n",
    "    random.shuffle(data)\n",
    "    return pd.DataFrame.from_records(data, columns=['id', 'Text', 'Label'], index='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 1999 entries, cv163_10110 to cv969_13250\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   Text    1999 non-null   object\n",
      " 1   Label   1999 non-null   int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 46.9+ KB\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>cv444_9975</th>\n",
       "      <td>the makers of spawn have created something alm...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cv925_9459</th>\n",
       "      <td>everybody in this film's thinking of alicia . ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cv634_11989</th>\n",
       "      <td>i didn't come into city of angels expecting gr...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cv338_8821</th>\n",
       "      <td>leonardo decaprio ( what's eating gilbert grap...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cv100_12406</th>\n",
       "      <td>warning : spoilers are included in this review...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cv332_16307</th>\n",
       "      <td>like the wonderful 1990 drama , \" awakenings ,...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cv675_22871</th>\n",
       "      <td>have you ever been in an automobile accident w...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cv254_5870</th>\n",
       "      <td>even the best comic actor is at the mercy of h...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cv717_17472</th>\n",
       "      <td>starring arnold schwarzenegger ; danny devito ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cv957_8737</th>\n",
       "      <td>capsule : the best place to start if you're a ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                          Text  Label\n",
       "id                                                                   \n",
       "cv444_9975   the makers of spawn have created something alm...      0\n",
       "cv925_9459   everybody in this film's thinking of alicia . ...      0\n",
       "cv634_11989  i didn't come into city of angels expecting gr...      0\n",
       "cv338_8821   leonardo decaprio ( what's eating gilbert grap...      1\n",
       "cv100_12406  warning : spoilers are included in this review...      0\n",
       "cv332_16307  like the wonderful 1990 drama , \" awakenings ,...      1\n",
       "cv675_22871  have you ever been in an automobile accident w...      0\n",
       "cv254_5870   even the best comic actor is at the mercy of h...      0\n",
       "cv717_17472  starring arnold schwarzenegger ; danny devito ...      0\n",
       "cv957_8737   capsule : the best place to start if you're a ...      1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset_archive('data/review_polarity.zip')\n",
    "print(dataset.info())\n",
    "dataset.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature extraction\n",
    "\n",
    "To use ML techniques we need to transform the textual representation into a set of features, and for that we can use the infrastructure provided by [scikit](https://scikit-learn.org/stable/modules/feature_extraction.html#text-feature). For this example I used the [sklearn.feature_extraction.text.CountVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer), but you can try also the [Tf–idf term weighting](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html#sklearn.feature_extraction.text.TfidfVectorizer) based extractor.\n",
    "\n",
    "To limit the number of features we can set the parameter `max_features` to the `CountVectorizer` constructor (the set of all features might be unmanageable).\n",
    "\n",
    "We can ignore stopwords by using the `stop_words` parameter, below we'll use the data from the downloaded NLTK corpus.\n",
    "\n",
    "N-grams can be considered by specifying the `ngram_range` parameter. E.g `(1,3)` uses length 1, 2, and 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 5.8 s\n",
      "Wall time: 5.92 s\n",
      "(1999, 100000)\n"
     ]
    }
   ],
   "source": [
    "vectorizer = sklearn.feature_extraction.text.CountVectorizer(\n",
    "    stop_words=nltk.corpus.stopwords.words('english'),\n",
    "    max_features=100000,\n",
    "    ngram_range=(1,3)\n",
    ")\n",
    "\n",
    "%time fmatrix = vectorizer.fit_transform(dataset['Text'])\n",
    "\n",
    "print(fmatrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The default tokeniser is the regular expression `(?u)\\b\\w\\w+\\b` (`(?u)` switches on the `re.U (re.UNICODE)` flag), but we can also use one of the [NLTK tokenisers](https://www.nltk.org/api/nltk.tokenize.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\black\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\feature_extraction\\text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "C:\\Users\\black\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", 'could', 'might', 'must', \"n't\", 'need', 'sha', 'wo', 'would'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1999, 1000)\n"
     ]
    }
   ],
   "source": [
    "small_vectorizer = sklearn.feature_extraction.text.CountVectorizer(\n",
    "    stop_words=nltk.corpus.stopwords.words('english'),\n",
    "    max_features=1000,\n",
    "    ngram_range=(1,3),\n",
    "    tokenizer=nltk.tokenize.word_tokenize\n",
    ")\n",
    "\n",
    "small_matrix = small_vectorizer.fit_transform(dataset['Text'])\n",
    "\n",
    "print(small_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification and Evaluation\n",
    "\n",
    "Below you'll find an example using the [logistic regression](https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression) classifier, with the corresponding C parameter tuning using the `lbfgs` solver:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 19.6 s\n",
      "Wall time: 19.7 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>C</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.01</td>\n",
       "      <td>0.838914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.05</td>\n",
       "      <td>0.842915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.25</td>\n",
       "      <td>0.845917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.50</td>\n",
       "      <td>0.843416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.842916</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      C  Accuracy\n",
       "0  0.01  0.838914\n",
       "1  0.05  0.842915\n",
       "2  0.25  0.845917\n",
       "3  0.50  0.843416\n",
       "4  1.00  0.842916"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "base_estimator = LogisticRegression(solver='sag')\n",
    "\n",
    "param_grid = {'C': [0.01, 0.05, 0.25, 0.5, 1]}  # possible options for the C parameter of the regression\n",
    "\n",
    "clf = GridSearchCV(base_estimator, param_grid=param_grid)\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    %time clf.fit(fmatrix, dataset['Label'])\n",
    "\n",
    "pd.concat([pd.DataFrame(clf.cv_results_[\"params\"]),pd.DataFrame(clf.cv_results_[\"mean_test_score\"], columns=[\"Accuracy\"])],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train with whole dataset\n",
    "\n",
    "Once you selected the parameter you can prepare the model for classifying unseen data. Usually you prepare the model for deployment by using the whole dataset (beware of overfitting, though)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 875 ms\n",
      "Wall time: 872 ms\n"
     ]
    }
   ],
   "source": [
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    %time lr_full = LogisticRegression(C=1, solver='sag').fit(fmatrix, dataset['Label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trained models can be saved for later deployment using Python libraries for serialisation. Scikit documentation suggests to use [joblib](https://scikit-learn.org/stable/modules/model_persistence.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(C=1, solver=&#x27;sag&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(C=1, solver=&#x27;sag&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(C=1, solver='sag')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "joblib.dump(lr_full, 'my_lr_classifier.joblib')\n",
    "joblib.dump(vectorizer, 'my_full_vectorizer.joblib')\n",
    "lr_copy = joblib.load('my_lr_classifier.joblib')\n",
    "lr_copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the model for prediction\n",
    "\n",
    "To classify new instance the features must be aliged to the ones used for training. To this end you use the `transform` method of the corresponding vectoriser (the `fit` phase is the one where the features are selected): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, 1], dtype=int64)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_example = vectorizer.transform(['drill', 'good drill', 'crap film', 'excellent one'])\n",
    "lr_full.predict(new_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the features of the new data. To understand which features are in the example we need to consider the list of feature names in the vectoriser (the method `get_feature_names()`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drill (0,23230)=1 \n",
      "drill (1,23230)=1 \n",
      "good (1,37750)=1 \n",
      "crap (2,17940)=1 \n",
      "crap film (2,17943)=1 \n",
      "film (2,30208)=1 \n",
      "excellent (3,27495)=1 \n",
      "excellent one (3,27511)=1 \n",
      "one (3,62006)=1 \n"
     ]
    }
   ],
   "source": [
    "features = vectorizer.get_feature_names_out()\n",
    "for row, col in zip(*new_example.nonzero()):\n",
    "    print('{} ({},{})={} '.format(features[col], row, col, new_example[row,col]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try a different classifier\n",
    "\n",
    "Select a different classifier and verify whether you can obtain a better accuracy. With textual data [naïve Bayes](https://scikit-learn.org/stable/modules/naive_bayes.html#naive-bayes) and [support vector machine (SVM)](https://scikit-learn.org/stable/modules/svm.html#svm) are often used, but you can also train and use [deep learning models](https://developers.google.com/machine-learning/guides/text-classification/step-4).\n",
    "\n",
    "Comment on your experiments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before applying our classifier to custom data, let us first test it on the existing dataset, by dividing the original dataset in two portions, the training set a validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the classifier: 86.25%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Splitting the dataset into training and test sets (80% training, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(dataset['Text'], dataset['Label'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Vectorizing the training set\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train_vectorized = vectorizer.fit_transform(X_train)\n",
    "\n",
    "# Training the SVM model using only the training set\n",
    "svm = SVC(kernel='linear')\n",
    "svm.fit(X_train_vectorized, y_train)\n",
    "\n",
    "# Vectorizing the test set\n",
    "X_test_vectorized = vectorizer.transform(X_test)\n",
    "\n",
    "# Predicting labels for the test set\n",
    "predictions = svm.predict(X_test_vectorized)\n",
    "\n",
    "# Calculating accuracy by comparing predicted labels to true labels\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(f\"Accuracy of the classifier: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the computation displays, the classifier has classified reviews correctly with a rate of 86.25%. This is not a bad score, therefore we can proceed applying it on custom data of our choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review: good film --> Predicted Label: 1\n",
      "Review: crap film --> Predicted Label: 0\n",
      "Review: I find it very interesting --> Predicted Label: 0\n",
      "Review: I will for sure watch it again --> Predicted Label: 1\n",
      "Review: how can you keep watching this for 2 hours straight, boredom in its purest for --> Predicted Label: 0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "#First let's define what reviews we want to proof, in this case we specify custom ones\n",
    "test_reviews = [\"good film\", \"crap film\", \"I find it very interesting\", \"I will for sure watch it again\"]\n",
    "\n",
    "#We use a TfidVectorizer to vectorize the dataset\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_vectorized = vectorizer.fit_transform(dataset['Text'])\n",
    "\n",
    "# Training the SVM model (notice that since we want to test custom reviews we can use the whole dataset here, without splitting)\n",
    "svm = SVC(kernel='linear')\n",
    "svm.fit(X_vectorized, dataset['Label'])\n",
    "\n",
    "# Predicting the sentiment of provided reviews\n",
    "test_reviews = [\"good film\", \"crap film\", \"I find it very interesting\", \"I will for sure watch it again\", \"how can you keep watching this for 2 hours straight, boredom in its purest for\"]\n",
    "test_reviews_vectorized = vectorizer.transform(test_reviews)\n",
    "predictions = svm.predict(test_reviews_vectorized)\n",
    "\n",
    "# Display predictions for test reviews\n",
    "for review, prediction in zip(test_reviews, predictions):\n",
    "    print(f\"Review: {review} --> Predicted Label: {prediction}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By looking at the reviews, one would conclude:\n",
    "- Positive Reviews:\n",
    "1. good film\n",
    "2. I find it very interesting\n",
    "3. I will for sure watch it again\n",
    "- Negative Reviews:\n",
    "1. crap film\n",
    "2. how can you keep watching this for 2 hours straight, boredom in its purest for\n",
    "\n",
    "The classifier predicted the label correctly 4/5 times. However, in this case we are considering a custom and small sample of tweets."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e238ed82568763522dac3f9ad71a9c8d6b9500024dfa12bd64b36cc255a6bffe"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
